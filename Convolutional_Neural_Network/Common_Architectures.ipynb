{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Common Architectures in Convolutional Neural Networks\n",
    "\n",
    "### Amin Zabardast"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Quick review: Convolutional Neural Network\n",
    "\n",
    "Simply described, a Convolutional Neural Network (CNN or ConvNet) is a type of Feed-Forward Artificial Neural Network in which - some of - hidden layers are structured in such a way that their output on a given input is a convoluted version of input data using a kernel.\n",
    "\n",
    "These kind of Neural Networks are powerful in Image Analysis applications because any pixel in an image will be correlated to other pixels in its vicinity and this correlation would be inversely proportional to the distance of pixels. Convolution operation will gather information from a pixel's vicinity to assign some properties to the pixel in form of numerical value. These Numerical values can form low-level features such as edges and corners in first layers and high-level features such as faces and cars in deeper layers."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Architectures are unavoidable\n",
    "\n",
    "Creating a ConvNet, even the simplest one, requires some designing effort. The number of hidden layers, convolution kernel size, max pooling step size are just a few of the properties a ConvNet will have and from this step onward, everything will get more complicated.\n",
    "\n",
    "The design of a ConvNet will have a direct effect on its capabilities. For example, shallow Neural Networks will not have enough non-linearity power to classify a big data space, but on the other hand, very deep neural network are hard to train because of vanishing gradient problem. This will result in an optimization problem. In other words, the architecture of the network will have a big effect on its capabilities. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Case Studies\n",
    "\n",
    "In this document, we will be analyzing some of the ConvNet Architectures which had the highest performance in their relative time. Many of these architectures have been modeled in common platforms or they have finely tuned programs written in lower-level languages such as C++ to boost their performances.\n",
    "\n",
    "* LeNet (1998)\n",
    "* AlexNet (2012)\n",
    "* ZF Net (2013)\n",
    "* VGG (2014)\n",
    "* GoogleNet (2014)\n",
    "* ResNet (2015)\n",
    "\n",
    "Studying these architectures will result in a better understanding of main challenges in Convolutional Neural Networks and provides insights to create a better performing architecture."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## LeNet\n",
    "[[LeCun et al., 1998]](http://yann.lecun.com/exdb/publis/pdf/lecun-01a.pdf)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "LeCun had the idea of using a neural network to perform convolution on a smaller region of an image like a convolution instead of applying it over a whole image using a fully connected network and successfully tested it as digit recognition network for postal codes [(LeCun et al., 1989)](http://yann.lecun.com/exdb/publis/pdf/lecun-90c.pdf). The concept of Convolutional layer followed by a Pooling layer is at the heart of LeNet Models. This architecture will be followed by some fully connected (FC) layers for classification.\n",
    "\n",
    "The convolutional layers usually use a $5\\times5$ filer but there are examples of different sizes and pooling layers usually use a $2\\times2$ max pooling or non ad all, however there are examples of $4\\times4$ max pooling layers when in comes to large images."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img align=\"center\" src=\"imgs/img1.png\"/>\n",
    "<p>Image 1: Architecture of LeNet 5 (Image credits: LeCun, 1998)</p>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "LeNet is the simplest form of a convolutional neural network with a quite shallow architecture. However, the authors of the paper are quite successful to use this architecture in character recognition and mention that this learning architecture is applicable with a wide range of pattern recognition problems."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## AlexNet\n",
    "[[Krizhevsky et al., 2012]](https://papers.nips.cc/paper/4824-imagenet-classification-with-deep-convolutional-neural-networks.pdf)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The article titled \"ImageNet Classification with Deep Convolutional\n",
    "Neural Networks\", Alex Krizhevsky, Ilya Sutskever, and Geoffrey Hinton created a Deep Convolutional Neural Network which won the 2012 \"ImageNet Large Scale Visual Recognition Challenge\" (ILSVRC) with a large score difference. It is safe to say that ConvNets become the dominant high scores in the competition ranking after 2012."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this paper, authors created a relatively simple ConvNet layout. This network consisted of 5 Convolutional Layers, 3 max pooling layers, and a fully connected network in the end."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img align=\"center\" src=\"imgs/img2.png\"/>\n",
    "<p>Image 2: Architecture of AlexNet (Image credits: Krizhevsky, 2012)</p>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The main contributions of AlexNet can be summed up as follow:\n",
    "\n",
    "- First use of rectified linear unit (ReLU): Before this paper, the most non-linearity function used was either tangent hyperbolic or sigmoid function. In this paper, authors showed there is a noticeable difference in learning speed when using ReLU in comparison with other non-linearity functions.\n",
    "\n",
    "- Data augmentation: The authors utilized data augmentation technique to combat overfitting. They utilized a python code which generated transformed images using CPU as a computational resource while GPU is training a batch, which, to use their words, \"... data augmentation schemes are, in effect, computationally free.\"\n",
    "\n",
    "- Drop out layers: Implemented dropout layers in order to combat the problem of overfitting to the training data.\n",
    "\n",
    "- Utilizing momentum in stochastic gradient descent: They use stochastic gradient descent with a momentum of 0.9 to have a fast learning process.\n",
    "\n",
    "- Training using multiple GPUs: AlexNet is designed to use multiple GPUs so they may share the size of network. According to authors, the GPUs they used are designed to be able to comunicate with eachother without using the host machines memory as a buffering zone."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It is fair to say that CovNets gained so much attention after the success of AlexNet. The size of AlexNet is considered large compared to its depth. AlexNet trained 60 million parameters and 650 thousand neurons with 15 million images in 5 to 6 days."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ZF Net\n",
    "[[Zeiler & Fergus, 2013]](https://arxiv.org/pdf/1311.2901v3.pdf)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After the success of AlexNet in 2012 ILSVR, many others entered 2013 ILSVRC competition with ConvNets. The winner of 2013 ILSVRC was another ConvNet structure created by Zeiler and Fergus, commonly known as ZF Net. There is not much to discuss around ZF Net since it's structure is highly based on AlexNet from 2012. However, in their article, Zeiler and Fergus introduce the concept of Decovnet which will be discussed here."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img align=\"center\" src=\"imgs/img3.png\"/>\n",
    "<p>Image 3: A ConvNet layer (right) along side a Deconv layer (left). (Image credits: Zeiler & Fergus, 2013)</p>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "They designed their ConvNet in a particular manner. In their design, for any ConvNet layer, there is a parallel Deconvnet which will be responsible for illustrating the specific activation pattern of the neuron. They achieve this by storing the activations (9 highest activations) of the image and set rest of them to 0. Afterwards, they go through the Deconve layer which involves un-pooling (reverse max pooling) mechanism to create a rendition of the activation in a manner that illuminates the inner working of each layer."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img align=\"center\" width=\"400\" src=\"imgs/img4.png\"/>\n",
    "<p>Image 4: A small example from the result produced by Deconv layer. (Image credits: Zeiler & Fergus, 2013)</p>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## VGG Net\n",
    "[[Simonyan & Zisserman, 2015]](https://arxiv.org/pdf/1409.1556v6.pdf)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "One of the highest scores in ILSVRC 2014 (but not the winner) was a simple but deep model introduced by Karen Simonyan and Andrew Zisserman. This model is a ConvNet with 19 layers of depth which only uses 3 by 3 kernels and 2 by 2 max pooling layers."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img align=\"center\" width=\"400\" src=\"imgs/img5.png\"/>\n",
    "<p>Image 5: A table containing different configurations of VGG Net. Configuration D produced the best result. (Image credits: Simonyan & Zisserman, 2015)</p>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In their article, the authors discuss that although they only use 3 by 3 kernels for convolution, but back to back usage of 3 by 3 convolutional layers will reproduce the same effect of using a 7 by 7 convolutional layer and their configuration needs less computation. They also argue that with more layer (using more ReLU) they have more non-linearity power which is an advantage."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This ConvNet was trained using batched gradient descent algorithm and it took 2 or 3 weeks to train on 4 GPUs. This paper is one of the first articles proposing to use a deep but simple architectures and their success and error rate supports this idea."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## GoogleNet\n",
    "[[Szegedy et al., 2015]](https://www.cv-foundation.org/openaccess/content_cvpr_2015/papers/Szegedy_Going_Deeper_With_2015_CVPR_paper.pdf)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The winner of ILSVRC 2014 was a deep ConvNet named GoogleNet. Although, like the previous ConvNet example, this architecture with 22 layers is also considered a deep ConvNet, but it won't be called a \"simple architecture\". The authors discuss which this architecture have a noticeable difference in resource consumption, i.e. memory and computation."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img align=\"center\" src=\"imgs/img6.png\"/>\n",
    "<p>Image 6: Google Net Architecture. (Image credits: Szegedy et al., 2015)</p>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This architecture is a modular design and each module of this architecture is called an Inception module. The idea behind this design is to be able to do all convolutional and pooling operations in one layer instead of having dedicated layers of convolution or max pooling like what we have observed until now."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img align=\"center\" width=400 src=\"imgs/img7.png\"/>\n",
    "<p>Image 7: Inception Module. (Image credits: Szegedy et al., 2015)</p>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The reason behind this is to introduce a depth growth control. Imagine performing a 5 by 5 convolution followed by a 3 by 3 convolution this will result in a rapid growth of depth in output data. What authors wanted was to be able to use different kernel sizes and use different kernels contribution and power but not have to worry about the quick growth of depth. What they decided was to run all the kernels they want (in a module) but in parallel and then concatenate the result. This will not have a rapid growth of depth but allows the network to utilize the power of different convolutional kernels. Watch this [video](https://www.youtube.com/watch?v=KfV8CJh7hE0) if you want a bit more explanations of its inner working."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The authors used 9 inception models in their paper with more than 100 layers combined. This becomes more interesting since this architecture has less than 10 times parameters in comparison with AlexNet.\n",
    "\n",
    "GoogleNet was the first architecture proposing anything except stacking layers of convolution/poolin upon each other."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ResNet\n",
    "[[He et al., 2015]](https://arxiv.org/pdf/1512.03385v1.pdf)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ResNet is the result of research from Microsoft Research Asia and it is the winner of ILSVRC 2015 with a small error rate of 3.6%. This error rate is on average, less than a human. ResNet is an extremely deep network of whopping 152 layers. One of the main issues using such a deep network is the flow of gradient in backpropagation mechanism. The authors of this paper used a method to facilitate the flow of gradient."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img align=\"center\" src=\"imgs/img8.png\"/>\n",
    "<p>Image 8: A ResNet with 34 parameter layers. (Image credits: He et al., 2015)</p>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "How feed forward usually works is as if you give data $x$ to a layer and get $f(x)$ from the layer. This means $f(x)$ is a completely different from input, $x$. But in \"Residual Network\" $h(x)$, as output of \"residual block\", would be $f(x)+x$. This output is a slight change from the block input. The authors believe this would facilitate the flow of gradient which in turn all0ws ResNet to have an \"ultra\" deep architecture. This architecture uses Stochastic Gradient Methods with Momentum of 0.9 and does not utilize drop out methods."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ResNet was the most successful ConvNet architecture so far and it was slightly improved by combining it with Inception module idea of GoogleNet [(Szegedy et al., 2016)](https://arxiv.org/pdf/1602.07261.pdf)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Comparison"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There is a recent great published article titled \"An Analysis Of Deep Neural Network Models For Practical Applications\" [(Canziani et al., 2017)](https://arxiv.org/pdf/1605.07678.pdf) which compares existing deep network architectures from the perspective of their applications, as the title suggests."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img align=\"center\" src=\"imgs/img9.png\"/>\n",
    "<p>Image 9:  (Image credits: He et al., 2015)</p>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I highly recommend reading the original article for more indepth descussion. However a prompt analysis of their results could be:\n",
    "- AlexNet: Memory heavy and lower accuracy\n",
    "- VGG: Highest memory use and most operations needed\n",
    "- GoogLeNet: Most efficient and low memory consumption\n",
    "- ResNet: Moderate efficiency depending design and highest accuracy"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

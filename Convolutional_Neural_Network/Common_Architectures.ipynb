{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Common Architectures in Convolutional Neural Networks\n",
    "\n",
    "### Amin Zabardast"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Quick review: Convolutional Neural Network\n",
    "\n",
    "Simply described, a Convolutional Neural Network (CNN or ConvNet) is a type of Feed-Forward Artificial Neural Network in which - some of - hidden layers are structured in such a way that their output on a given input is a convoluted version of input data using a kernel.\n",
    "\n",
    "These kind of Neural Networks are powerful in Image Analysis applications because any pixel in an image will be correlated to other pixels in its vicinity and this correlation would be inversely proportional to the distance of pixels. Convolution operation will gather information from a pixel's vicinity to assign some properties to the pixel in form of numerical value. These Numerical values can form low-level features such as edges and corners in first layers and high-level features such as faces and cars in deeper layers."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Architectures are unavoidable\n",
    "\n",
    "Creating a ConvNet, even the simplest one, requires some designing effort. The number of hidden layers, convolution kernel size, max pooling step size are just a few of the properties a ConvNet will have and from this step onward, everything will get more complicated.\n",
    "\n",
    "The design of a ConvNet will have a direct effect on its capabilities. For example, shallow Neural Networks will not have enough non-linearity power to classify a big data space, but on the other hand, very deep neural network are hard to train because of vanishing gradient problem. This will result in an optimization problem. In other words, the architecture of the network will have a big effect on its capabilities. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Case Studies\n",
    "\n",
    "In this document, we will be analyzing some of the ConvNet Architectures which had the highest performance in their relative time. Many of these architectures have been modeled in common platforms or they have finely tuned programs written in lower-level languages such as C++ to boost their performances.\n",
    "\n",
    "* LeNet (1998)\n",
    "* AlexNet (2012)\n",
    "* ZF Net (2013)\n",
    "* VGG (2014)\n",
    "* GoogleNet (2014)\n",
    "* ResNet (2015)\n",
    "\n",
    "Studying these architectures will result in a better understanding of main challenges in Convolutional Neural Networks and provides insights to create a better performing architecture."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## LeNet\n",
    "[[LeCun et al., 1998]](http://yann.lecun.com/exdb/publis/pdf/lecun-01a.pdf)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "LeCun had the idea of using a neural network to perform convolution on a smaller region of an image like a convolution instead of applying it over a whole image using a fully connected network and successfully tested it as digit recognition network for postal codes [(LeCun et al., 1989)](http://yann.lecun.com/exdb/publis/pdf/lecun-90c.pdf). The concept of Convolutional layer followed by a Pooling layer is at the heart of LeNet Models. This architecture will be followed by some fully connected (FC) layers for classification.\n",
    "\n",
    "The convolutional layers usually use a $5\\times5$ filer but there are examples of different sizes and pooling layers usually use a $2\\times2$ max pooling or non ad all, however there are examples of $4\\times4$ max pooling layers when in comes to large images."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img align=\"center\" src=\"imgs/img1.png\"/>\n",
    "<p>Image 1: Architecture of LeNet 5 (Image credits: LeCun, 1998)</p>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "LeNet is the simplest form of a convolutional neural network with a quite shallow architecture. However, the authors of the paper are quite successful to use this architecture in character recognition and mention that this learning architecture is applicable with a wide range of pattern recognition problems."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## AlexNet\n",
    "[[Krizhevsky et al., 2012]](https://papers.nips.cc/paper/4824-imagenet-classification-with-deep-convolutional-neural-networks.pdf)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The article titled \"ImageNet Classification with Deep Convolutional\n",
    "Neural Networks\", Alex Krizhevsky, Ilya Sutskever, and Geoffrey Hinton created a Deep Convolutional Neural Network which won the 2012 \"ImageNet Large Scale Visual Recognition Challenge\" (ILSVRC) with a large score difference. It is safe to say that ConvNets become the dominant high scores in the competition ranking after 2012."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this paper, authors created a relatively simple ConvNet layout. This network consisted of 5 Convolutional Layers, 3 max pooling layers, and a fully connected network in the end."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img align=\"center\" src=\"imgs/img2.png\"/>\n",
    "<p>Image 2: Architecture of AlexNet (Image credits: Krizhevsky, 2012)</p>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The main contributions of AlexNet can be summed up as follow:\n",
    "\n",
    "- First use of rectified linear unit (ReLU): Before this paper, the most non-linearity function used was either tangent hyperbolic or sigmoid function. In this paper, authors showed there is a noticeable difference in learning speed when using ReLU in comparison with other non-linearity functions.\n",
    "\n",
    "- Data augmentation: The authors utilized data augmentation technique to combat overfitting. They utilized a python code which generated transformed images using CPU as a computational resource while GPU is training a batch, which, to use their words, \"... data augmentation schemes are, in effect, computationally free.\"\n",
    "\n",
    "- Drop out layers: Implemented dropout layers in order to combat the problem of overfitting to the training data.\n",
    "\n",
    "- Utilizing momentum in stochastic gradient descent: They use stochastic gradient descent with a momentum of 0.9 to have a fast learning process.\n",
    "\n",
    "- Training using multiple GPUs: AlexNet is designed to use multiple GPUs so they may share the size of network. According to authors, the GPUs they used are designed to be able to comunicate with eachother without using the host machines memory as a buffering zone."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It is fair to say that CovNets gained so much attention after the success of AlexNet. The size of AlexNet is considered large compared to its depth. AlexNet trained 60 million parameters and 650 thousand neurons with 15 million images in 5 to 6 days."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ZF Net\n",
    "[[Zeiler & Fergus, 2013]](https://arxiv.org/pdf/1311.2901v3.pdf)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After the success of AlexNet in 2012 ILSVR, many others entered 2013 ILSVRC competition with ConvNets. The winner of 2013 ILSVRC was another ConvNet structure created by Zeiler and Fergus, commonly known as ZF Net. There is not much to discuss around ZF Net since it's structure is highly based on AlexNet from 2012. However, in their article, Zeiler and Fergus introduce the concept of Decovnet which will be discussed here."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img align=\"center\" src=\"imgs/img3.png\"/>\n",
    "<p>Image 3: A ConvNet layer (right) along side a Deconv layer (left). (Image credits: Zeiler & Fergus, 2013)</p>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "They designed their ConvNet in a particular manner. In their design, for any ConvNet layer, there is a parallel Deconvnet which will be responsible for illustrating the specific activation pattern of the neuron. They achieve this by storing the activations (9 highest activations) of the image and set rest of them to 0. Afterwards, they go through the Deconve layer which involves un-pooling (reverse max pooling) mechanism to create a rendition of the activation in a manner that illuminates the inner working of each layer."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img align=\"center\" width=\"400\" src=\"imgs/img4.png\"/>\n",
    "<p>Image 4: A small example from the result produced by Deconv layer. (Image credits: Zeiler & Fergus, 2013)</p>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## VGG Net\n",
    "[[Simonyan & Zisserman, 2015]](https://arxiv.org/pdf/1409.1556v6.pdf)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "One of the highest scores in ILSVRC 2014 (but not the winner) was a simple but deep model introduced by Karen Simonyan and Andrew Zisserman. This model is a ConvNet with 19 layers of depth which only uses 3 by 3 kernels and 2 by 2 max pooling layers."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img align=\"center\" width=\"400\" src=\"imgs/img5.png\"/>\n",
    "<p>Image 5: A table containing different configurations of VGG Net. Configuration D produced the best result. (Image credits: Simonyan & Zisserman, 2013)</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
